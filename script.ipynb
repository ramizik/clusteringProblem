{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import make_blobs as blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have CSV files named data1.csv, data2.csv, etc., in a folder named 'data'\n",
    "path = \"Data/03-11/*.csv\" \n",
    "columns = [' Source Port', ' Destination Port', ' Protocol',\n",
    "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Flow Bytes/s',\n",
    "       ' Flow Packets/s',  ' Inbound', ' Label']\n",
    "all_files = glob.glob(path)\n",
    "df_list = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for f in range(len(all_files)):\n",
    "    df = pd.read_csv(all_files[f],usecols = columns,low_memory = False)\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df1 = pd.concat(df_list, ignore_index=True)\n",
    "print(\"time_taken:\",datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have CSV files named data1.csv, data2.csv, etc., in a folder named 'data'\n",
    "path = \"Data/01-12/*.csv\" \n",
    "columns = [' Source Port', ' Destination Port', ' Protocol',\n",
    "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Flow Bytes/s',\n",
    "       ' Flow Packets/s',  ' Inbound', ' Label']\n",
    "all_files = glob.glob(path)\n",
    "df_list2 = []\n",
    "start_time = datetime.now()\n",
    "for f in range(len(all_files)):\n",
    "    df = pd.read_csv(all_files[f],usecols = columns,low_memory = False)\n",
    "    df_list2.append(df)\n",
    "\n",
    "combined_df2 = pd.concat(df_list2, ignore_index=True)\n",
    "print(\"time_taken:\",datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac098fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([combined_df2,combined_df1], ignore_index = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[' Label'].nunique())\n",
    "print(df[' Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ee44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'product_type': ['sandal', 'shoes', 'vest', 'tshirt', 'shoes', 'sandal'],\n",
    "    'price': [25, 60, 30, 15, 75, 40]\n",
    "})\n",
    "\n",
    "# Define a mapping dictionary\n",
    "category_map = {\n",
    "    'sandal': 'footwear',\n",
    "    'shoes': 'footwear',\n",
    "    'vest': 'apparel',\n",
    "    'tshirt': 'apparel'\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new 'category' column\n",
    "df['category'] = df['product_type'].map(category_map)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be891c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0158e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data\n",
    "X = df.iloc[:,0:8]\n",
    "y = df[' Label']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.dropna()\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ff89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a large dataset (e.g., 50 million samples)\n",
    "n_samples = 50000000\n",
    "n_features = 10\n",
    "n_clusters = 4\n",
    "columns = []   # fill the names\n",
    "print(\"Generating large dataset...\")\n",
    "X, y = blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state = 24)\n",
    "\n",
    "print(\"data generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting MiniBatchKMeans...\")\n",
    "minibatch_kmeans = MiniBatchKMeans(init='k-means++',n_clusters=n_clusters, batch_size=10000)\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_kmeans.fit(X)\n",
    "time_taken = time.time() - start_time\n",
    "\n",
    "# Get cluster assignments\n",
    "labels = minibatch_kmeans.predict(X)\n",
    "kmeans_inertia = minibatch_kmeans.inertia_\n",
    "print(\"Clustering complete.\")\n",
    "#print(\"Cluster centers:\", minibatch_kmeans.cluster_centers_)\n",
    "print(\"First 20 labels:\", labels[:10])\n",
    "print(f\"Mini Batch K-Means training time: {time_taken:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clustering results\n",
    "\n",
    "# plot: Mini Batch K-Means\n",
    "plt.scatter(X[:, 0], X[:, 1], c=minibatch_kmeans.labels_, s=1, cmap='viridis')\n",
    "plt.scatter(minibatch_kmeans.cluster_centers_[:, 0], minibatch_kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376da59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "#df = pd.DataFrame(X, columns=['feature_1', 'feature_2','feature_3', 'feature_4' ])\n",
    "\n",
    "# 4. Add the cluster labels to your original DataFrame\n",
    "df['cluster'] = labels\n",
    "df['Original Clusters'] = y\n",
    "\n",
    "print(\"Original Data with Cluster Assignments:\")\n",
    "print(df.head())\n",
    "# 5. Find points belonging to a specific cluster (e.g., cluster 2)\n",
    "cluster_2_points = df[df['cluster'] == 2]\n",
    "\n",
    "print(\"\\nPoints assigned to Cluster 2:\")\n",
    "print(cluster_2_points.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "# Optional: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "# Clustering\n",
    "print(\"Running MiniBatchKMeans...\")\n",
    "kmeans = MiniBatchKMeans(n_clusters=4, batch_size=10000)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Add labels to original data\n",
    "df['cluster'] = kmeans.labels_\n",
    "df['y'] = y\n",
    "print(\"Done. Sample output:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on a sample first and \n",
    "#then assign clusters to the full dataset (much faster)\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Assume X is very large (numpy, dask, or on disk)\n",
    "\n",
    "# Step 1: Take a random sample\n",
    "sample_size = 100000\n",
    "idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "X_sample = X[idx]\n",
    "\n",
    "# Step 2: Train MiniBatchKMeans on the sample\n",
    "k = 4\n",
    "mbk = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42)\n",
    "mbk.fit(X_sample)\n",
    "\n",
    "# Step 3: Assign all points to nearest centroid\n",
    "labels = pairwise_distances_argmin(X, mbk.cluster_centers_)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2e617",
   "metadata": {},
   "source": [
    "In-Memory Clustering with MiniBatchKMeans\n",
    "Best for: Medium-large datasets that fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=100)\n",
    "labels = hdbscan_cluster.fit_predict(X)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "X_chunk = X[:batch_size,: n_features]\n",
    "type(X_chunk)\n",
    "X_chunk.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d7ce5",
   "metadata": {},
   "source": [
    "For very large datasets (out-of-core), one can load data in chunks and call kmeans.partial_fit(chunk) repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Parameters\n",
    "n_clusters = 4\n",
    "batch_size = 500000   # adjust depending on your RAM\n",
    "n_features = 10      # number of features in your dataset\n",
    "\n",
    "# Initialize MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n",
    "                         batch_size=batch_size,\n",
    "                         random_state=42)\n",
    "\n",
    "# Example: Simulating streaming data (you'd replace this with file/DB chunks)\n",
    "#n_samples = 1_000_000\n",
    "for i in range(0, len(X), batch_size):\n",
    "    # Generate a chunk of data (replace this with real chunk loading, e.g., from CSV)\n",
    "    X_chunk = X[:batch_size, :n_features]\n",
    "    \n",
    "    # Update clusters with the chunk\n",
    "    kmeans.partial_fit(X_chunk)\n",
    "\n",
    "print(\"Cluster centers shape:\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "# After training, you can assign labels for new data\n",
    "labels = kmeans.predict(X)\n",
    "print(\"Labels for new data:\", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01945d8",
   "metadata": {},
   "source": [
    "#### Clustering with Dask (out-of-core, single machine but larger-than-RAM)\n",
    "\n",
    "Dask lets you process datasets that don’t fit into memory by chunking them. If dask is not installed please install with the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9682818b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 25.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.47 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.87 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.47 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.47 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.47 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.47 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 52.57 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 24.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 36.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 30.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 30.37 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 1 3 2 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.cluster import KMeans\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Start a Dask client (optional, but good practice for distributed computing)\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Create some sample Dask array data\n",
    "# make sure that X is Dask array.\n",
    "X = da.random.random((1000000, 10), chunks=(100, 10))\n",
    "#x_dask = da.from_array(X, chunks=len(X) // 10)\n",
    "# Initialize and fit the KMeans model\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    " # Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# You can compute the labels to view them (if they are a Dask array)\n",
    "computed_labels = cluster_labels.compute()\n",
    "\n",
    "# Print a portion of the labels\n",
    "print(computed_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e37f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramodgupta/anaconda3/lib/python3.11/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 28.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m dask_df\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m      5\u001b[0m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m computed_labels\n\u001b[0;32m----> 6\u001b[0m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal clusters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m      7\u001b[0m pandas_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "#dask_df = dd.from_dask_array(dask_array, columns=['col1', 'col2', 'col3'])\n",
    "import dask.dataframe as dd\n",
    "dask_df = dd.from_dask_array(X)\n",
    "pandas_df = dask_df.compute()\n",
    "pandas_df['cluster'] = computed_labels\n",
    "pandas_df['Original clusters'] = y\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7d1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59f10d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 800 B </td>\n",
       "                        <td> 800 B </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10, 10) </td>\n",
       "                        <td> (10, 10) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 1 chunks in 3 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> int64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,60.000000)\">10</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<getitem, shape=(10, 10), dtype=int64, chunksize=(10, 10), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check it. \n",
    "\n",
    "import dask.array as da\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Assume X is very large (numpy, dask, or on disk)\n",
    "\n",
    "\n",
    "# Step 2: Train MiniBatchKMeans on the sample\n",
    "k = 10\n",
    "mbk = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42)\n",
    "\n",
    "# Assume X is a Dask array\n",
    "X = da.random.random((1000000, 10), chunks=(1_000_000, 20))\n",
    "\n",
    "# Train MiniBatchKMeans on a sample (in memory)\n",
    "sample = X[:100000].compute()\n",
    "mbk.fit(sample)\n",
    "\n",
    "# Assign clusters for large dataset in parallel with Dask\n",
    "def assign_clusters(chunk, centers):\n",
    "    from sklearn.metrics import pairwise_distances_argmin\n",
    "    return pairwise_distances_argmin(chunk, centers)\n",
    "\n",
    "labels = X.map_blocks(assign_clusters, mbk.cluster_centers_, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adf9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8126e854",
   "metadata": {},
   "source": [
    "#### Clustering with PySpark (distributed, multi-machine / cluster)\n",
    "\n",
    "PySpark handles billions of rows across a cluster.\n",
    "\n",
    "Use this if you have a Spark cluster or very large data (TB scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f955c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b599029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PandasToSpark\").getOrCreate()\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Suppose df is a Spark DataFrame\n",
    "assembler = VectorAssembler(inputCols=spark_df.columns, outputCol=\"features\")\n",
    "dataset = assembler.transform(spark_df)\n",
    "\n",
    "kmeans = KMeans(k=10, seed=42)\n",
    "model = kmeans.fit(dataset)\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8600f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the file \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.appName(\"LargeClustering\").getOrCreate()\n",
    "\n",
    "# Load your big dataset (CSV/Parquet etc.)\n",
    "df = spark.read.csv(\"big_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "feature_cols = df.columns  # or select subset of columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "dataset = assembler.transform(df).select(\"features\")\n",
    "\n",
    "# Run KMeans clustering\n",
    "kmeans = KMeans().setK(10).setSeed(42)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Assign cluster predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Show sample results\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "\n",
    "# Train KMeans on a sample (say 1% of data)\n",
    "sample_df = dataset.sample(withReplacement=False, fraction=0.01, seed=42)\n",
    "kmeans = KMeans().setK(10).setSeed(42)\n",
    "model = kmeans.fit(sample_df)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = np.array(model.clusterCenters())\n",
    "\n",
    "# UDF to assign nearest cluster\n",
    "def assign_cluster(point):\n",
    "    distances = np.linalg.norm(centers - np.array(point), axis=1)\n",
    "    return int(np.argmin(distances))\n",
    "\n",
    "assign_cluster_udf = udf(assign_cluster, IntegerType())\n",
    "\n",
    "# Apply to full dataset\n",
    "predictions = dataset.withColumn(\"cluster\", assign_cluster_udf(\"features\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f48db4",
   "metadata": {},
   "source": [
    "Add a final cluster visualization (e.g., 2D PCA/t-SNE projection of clusters) so you can actually see the separation?\n",
    "\n",
    "Let’s add a final visualization step so you can actually see your clusters in 2D.\n",
    "Since large datasets have many features, we’ll use PCA (fast) or t-SNE/UMAP (better separation but slower) for dimensionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23154751",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cluster Visualization Helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_clusters(X, labels, method=\"pca\", sample_size=5000):\n",
    "    \"\"\"\n",
    "    Visualize clusters in 2D using PCA or t-SNE.\n",
    "    \n",
    "    X : numpy array (or Dask array -> .compute())\n",
    "    labels : cluster assignments\n",
    "    method : 'pca' or 'tsne'\n",
    "    sample_size : number of points to plot\n",
    "    \"\"\"\n",
    "    # Subsample for plotting\n",
    "    if len(X) > sample_size:\n",
    "        idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X = X[idx]\n",
    "        labels = np.array(labels)[idx]\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'pca' or 'tsne'\")\n",
    "    \n",
    "    X_2d = reducer.fit_transform(X)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(X_2d[:,0], X_2d[:,1], c=labels, cmap=\"tab10\", s=10, alpha=0.6)\n",
    "    plt.title(f\"Cluster Visualization ({method.upper()})\")\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e9d90",
   "metadata": {},
   "source": [
    "How to Use After Pipeline\n",
    "Dask Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering\n",
    "X = da.random.random((200000, 20), chunks=(50000, 20))\n",
    "out = cluster_pipeline_dask_auto(X, k_range=[2,4,6,8,10], sample_size=50000)\n",
    "\n",
    "# Predict clusters for a sample\n",
    "sample = X[:5000].compute()\n",
    "labels = out[\"best_model\"].predict(sample)\n",
    "\n",
    "# Visualize clusters\n",
    "visualize_clusters(sample, labels, method=\"pca\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8901241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering\n",
    "spark_df = spark.read.csv(\"big_dataset.csv\", header=True, inferSchema=True)\n",
    "out = cluster_pipeline_spark_auto(spark_df, feature_cols=spark_df.columns, k_range=[2,4,6,8,10])\n",
    "\n",
    "# Collect small sample to driver for plotting\n",
    "sample_df = spark_df.sample(fraction=0.01, seed=42).toPandas()\n",
    "labels = out[\"best_model\"].transform(\n",
    "    assembler.transform(spark_df.sample(fraction=0.01, seed=42)).select(\"features\")\n",
    ").toPandas()[\"prediction\"].values\n",
    "\n",
    "# Visualize clusters\n",
    "visualize_clusters(sample_df.values, labels, method=\"tsne\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b218e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888d6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b734a4",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/mini-batch-k-means-clustering-algorithm-in-machine-learning\n",
    "\n",
    "https://medium.com/@2328247224/mini-batch-k-means-an-efficient-clustering-algorithm-for-large-datasets-30b71a701ccc\n",
    "\n",
    "https://medium.com/@tanvirhossen_29772/mini-batch-k-means-e3083cc765f5\n",
    "\n",
    "https://medium.com/@2328247224/mini-batch-k-means-an-efficient-clustering-algorithm-for-large-datasets-30b71a701ccc\n",
    "\n",
    "https://docs.w3cub.com/scikit_learn/auto_examples/cluster/plot_mini_batch_kmeans.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
